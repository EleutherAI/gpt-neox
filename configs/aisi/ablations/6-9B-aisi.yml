{
   "pipe_parallel_size": 1,
   "model_parallel_size": 1,

   "num_layers": 32,
   "hidden_size": 4096,
   "num_attention_heads": 32,
   "seq_length": 2048,
   "max_position_embeddings": 2048,
   "norm": "layernorm",
   "pos_emb": "rotary",
   "rotary_pct": 0.25,
   "no_weight_tying": true,
   "gpt_j_residual": true,
   "output_layer_parallelism": "column",

   "attention_config": [[["flash"], 32]],

   "scaled_upper_triang_masked_softmax_fusion": true,
   "bias_gelu_fusion": true,

   "optimizer": {
     "type": "Adam",
     "params": {
       "lr": 0.00012,
       "betas": [0.9, 0.95],
       "eps": 1.0e-8
     }
   },

   "min_lr": 0.000012,

   "zero_optimization": {
    "stage": 1,
    "allgather_partitions": true,
    "allgather_bucket_size": 1260000000,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 1260000000,
    "contiguous_gradients": true,
    "cpu_offload": false
  },

   "train_micro_batch_size_per_gpu": 21,
   "gradient_accumulation_steps": 1,
   "data_impl": "mmap",

   "checkpoint_activations": true,
   "checkpoint_num_layers": 1,
   "partition_activations": true,
   "synchronize_each_layer": true,

   "gradient_clipping": 1.0,
   "weight_decay": 0.1,
   "hidden_dropout": 0,
   "attention_dropout": 0,

   "precision": "bfloat16",

   "train_iters": 72660,
   "lr_decay_iters": 72660,
   "distributed_backend": "nccl",
   "lr_decay_style": "cosine",
   "warmup": 0.01,
   "checkpoint_factor": 1000,
  #  "extra_save_iters": [0,1,2,4,8,16,32,64,128,256,512], # Comment out during grid search
   "eval_interval": 72660,
   "eval_iters": 10,

   "log_interval": 1,
   "steps_per_print": 10,
   "wall_clock_breakdown": true,

  #  "tokenizer_type": "HFTokenizer",

   "save": "/checkpoints/6-9B-aisi",
   "load": "/checkpoints/6-9B-aisi",
}
